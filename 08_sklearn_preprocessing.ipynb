{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_sklearn_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfcJiPMDup9h/qkMd1NS4G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AudryBarimbane/Python_Machine_Learning/blob/main/08_sklearn_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD_3htp7W0pu"
      },
      "source": [
        "**PYTHON SKLEARN PRE-PROCESSING + PIPELINE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWqQYJlTfvzA"
      },
      "source": [
        "\n",
        "* Faire du pre-processing pour améliorer les performances en machine learning et en data science \n",
        "* Traiter des données avec LabelEncoder,OneHotEncoder,MinMaxScaler,StandardScaler,et d'autres transformers du module sklearn.preprocessing\n",
        "* Assembler plusieurs transformer avec le module Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kM4sISDeVFu"
      },
      "source": [
        "pre-processing :\n",
        "---------------\n",
        "Les algos de ML apprennent à partir  des données qui leur sont fournies,par conséquent si ces données sont de mauvaise qualité,incomplètes,erronnées ,redondantes,l'algo qui en résulte sera lui même assez mauvais ,puisqu'il est sensé reflèter ce qu'il voit dans les données, c'est pour cette raison qu'il est impératif de bien préparer nos données avant leur passage dans la ML.Il faut les nettoyer,les filtrer,les normaliser : c'est le pre-processing(prétraitement).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_JjYRJuW5id"
      },
      "source": [
        "\n",
        "Opérations de pre-processing :\n",
        "1.Encodage (convertir les données qualitatives en valeurs numériques)\n",
        "\n",
        "chien 0\n",
        "\n",
        "chat  1\n",
        "\n",
        "chien 0\n",
        "\n",
        "oiseau 2\n",
        "\n",
        "2.Normalisation(mettre sur une même échelle toutes les variables quantitatives ce qui facilite beaucoup l'apprentissage de la machine)\n",
        "\n",
        "2     0\n",
        "\n",
        "10    1\n",
        "\n",
        "4     0.25\n",
        "\n",
        "6      0.5\n",
        "\n",
        "\n",
        "3.Imputation(remplacer des données manquantes par certaines valeurs statistiques)\n",
        "\n",
        "2    2\n",
        "\n",
        "1    1\n",
        "\n",
        "3     3\n",
        "\n",
        "'nan'   2\n",
        "\n",
        "\n",
        "4.Sélection(sélection de variables qui utilise les tests statistiques comme le test de q2 pour sélectionner  les variables les plus utiles pour le développement d'un modèle )\n",
        "\n",
        "1  0                1\n",
        "\n",
        "2  1         =>     2\n",
        "\n",
        "3   0               3   \n",
        "\n",
        "4    0               4\n",
        "\n",
        "\n",
        "5.Extraction d'un modèle qui consiste à générer de nouvelles variables à partir d'informations cachées dans le dataset \n",
        "\n",
        "\n",
        "sklearn.preprocessing  :   encodage et normalisation\n",
        "\n",
        "\n",
        "sklearn.impute    :     imputation \n",
        "\n",
        "\n",
        "sklearn.feature  :   sélection \n",
        "\n",
        "\n",
        "sklearn.feature_extraction :  extraction \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y21zEOChi9dQ"
      },
      "source": [
        "sklearn.preprocessing :Le transformer \n",
        "\n",
        "  1.Les classes transformers\n",
        "  \n",
        "  2.Des simples routines ou des fonctions mathématiques "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cay4W8PEWpDq"
      },
      "source": [
        "import numpy as np \n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JXDuN-_WPXA"
      },
      "source": [
        "X = np.array(['Chat',\n",
        "              'Chien',\n",
        "              'Chat',\n",
        "              'Oiseau'])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwaPRNsSWu7M"
      },
      "source": [
        "**Transformer**\n",
        "\n",
        "Pour transformer les données de façon cohérente ,les transformers disposent de 2 méthodes:\n",
        "\n",
        " 1.méthode fit(Xtrain):développe une fonction de transformation à partir de Xtrain\n",
        " \n",
        " 2.méthode transform(X) : applique la transformation sur les données Xtrain ,Xtest et toutes les autres données futures.\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pr4O3TcXyke",
        "outputId": "f62f1cb6-c144-4306-fef6-f8630294dd1e"
      },
      "source": [
        "transformer = LabelEncoder()\n",
        "transformer.fit(X)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VyFoNIhYE6B",
        "outputId": "add2ec32-addd-43b6-ebe0-070439ba2932"
      },
      "source": [
        "transformer.transform(X)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdKKjpN2Yar8",
        "outputId": "71885324-477f-4827-ac37-5b67648dee6a"
      },
      "source": [
        "#méthode fit_transform combiner les méthodes fit et transform\n",
        "transformer.fit_transform(X)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecft9SBvY6LE"
      },
      "source": [
        "**Transform et Estimator**\n",
        "\n",
        "En pratique ,si l'on désire développer un modèle de Ml , on commence par diviser notre dataset en 2 parties(trainset et testset)\n",
        "\n",
        "(X,y) :(Xtrain ,ytrain) et (Xtest,ytest)\n",
        "\n",
        "Avec les données du trainset , nous développons une fonction de transformation(transform) avec la méthode fit_transform(),ce qui permet de traiter nos données ,pour ensuite entrainer un estimateur avec la méthode fit()\n",
        "\n",
        "Après cette étape ,nous pouvons utiliser le transformer(méthode transform()) et l'estimator(méthode predict()) tels que développés pour transformer les données du testset ,puis faire de nouvelles prédictions .\n",
        "\n",
        "En combinant , un transformer et un estimator : on obtient une pipeline(càd une chaine de transformation)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB8-lmo5Ym_5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}